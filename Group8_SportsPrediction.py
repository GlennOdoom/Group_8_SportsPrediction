# -*- coding: utf-8 -*-
"""Group8_SportsPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17rXLOMyktwbbsfbLucLZE7p9xPbU8oTw
"""

import pandas as pd
from google.colab import drive
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
drive.mount('/content/drive')

"""Load the initial dataset (players21)"""

players21=pd.read_csv('/content/drive/My Drive/Colab Notebooks/players_21.csv')

"""Getting a Birds-eye view of the dataset"""

players21.head()

players21.info(verbose=True,null_counts=True)

"""Dropping Irrelevant Columns based on domain knowledge"""

irrelevant_columns = ['player_url','player_face_url', 'club_logo_url','club_flag_url','nation_logo_url','nation_flag_url','nationality_id','nationality_name','nation_team_id','club_loaned_from','club_contract_valid_until','club_jersey_number']
players21.drop(columns=irrelevant_columns, inplace=True)

"""Remove columns with more than 30% missing values

"""

players21=players21.loc[:,players21.isnull().mean() <.3]

"""Separating data into numeric and non-numeric set."""

numeric_columns = players21.select_dtypes(include=['number'])  # This selects numeric columns
categorical_columns = players21.select_dtypes(exclude=['number'])  # This selects non-numeric (categorical) columns

numeric_columns

categorical_columns

"""Handling missing values"""

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
numeric_columns = pd.DataFrame(imputer.fit_transform(numeric_columns), columns=numeric_columns.columns)
numeric_columns.info()

categorical_columns.fillna(method='ffill', inplace=True)

from sklearn.preprocessing import LabelEncoder

# Create a LabelEncoder
label_encoder = LabelEncoder()

# Encode each categorical column
for column in categorical_columns:
    categorical_columns[column] = label_encoder.fit_transform(categorical_columns[column])

# Now you can access the encoded DataFrame

encoded=categorical_columns
# Display the first few rows of the encoded DataFrame
encoded.head(30)

"""Combine numeric and encoded categorical columns

"""

# Concatenate them row-wise
combined_df = pd.concat([numeric_columns, encoded],axis=1)

# Reset the index if needed
players21 = combined_df.reset_index(drop=True)

players21.info()

"""Preparing data for training using random forest regressor model and using future importance

"""

# Prepare the data for training
X = combined_df.drop('overall', axis=1)
y = combined_df['overall']

# Create a Random Forest Regressor model
random_forest = RandomForestRegressor()
random_forest.fit(X, y)

# Get feature importances
feature_importances = random_forest.feature_importances_

# Create a DataFrame to associate feature names with their importance scores
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})

# Sort the features by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Display the feature importances
print(feature_importance_df)

"""Selecting the 10 top features and scale them

"""

top_n = 10
top_features = feature_importance_df.head(top_n)['Feature'].tolist()

X = X[top_features]

scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# Display the scaled top features
X_scaled

y

"""Spliting the data to train and test"""

from sklearn.model_selection import train_test_split

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

from sklearn.model_selection import GridSearchCV, KFold
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, RandomForestRegressor
import xgboost as xgb

param_grid_gb = {
    'n_estimators': [50, 100, 150],
    'learning_rate': [0.01, 0.05],
    'max_depth': [2, 3, 4]
}

param_grid_ab = {
    'n_estimators': [50, 100, 150],
    'learning_rate': [0.01, 0.05, 0.1]
}

param_grid_rf = {
    'n_estimators': [50, 100, 150],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

param_grid_xgb = {
    'n_estimators': [50, 100, 150],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 4, 5]
}

gradient_boosting = GradientBoostingRegressor()
ada_boost = AdaBoostRegressor()
random_forest = RandomForestRegressor()
xg_boost = xgb.XGBRegressor()

"""Define cross-validation settings"""

cv = KFold(n_splits=5, shuffle=True, random_state=42)

"""Perform hyperparameter search with cross-validation for each regressor and fitting the models to train data

"""

grid_search_gb = GridSearchCV(gradient_boosting, param_grid=param_grid_gb, cv=cv, n_jobs=-1, scoring='neg_mean_absolute_error')
grid_search_ab = GridSearchCV(ada_boost, param_grid=param_grid_ab, cv=cv, n_jobs=-1, scoring='neg_mean_absolute_error')
grid_search_rf = GridSearchCV(random_forest, param_grid=param_grid_rf, cv=cv, n_jobs=-1, scoring='neg_mean_absolute_error')
grid_search_xgb = GridSearchCV(xg_boost, param_grid=param_grid_xgb, cv=cv, n_jobs=-1, scoring='neg_mean_absolute_error')

grid_search_gb.fit(X_train, y_train)

grid_search_ab.fit(X_train, y_train)

grid_search_rf.fit(X_train, y_train)

grid_search_xgb.fit(X_train, y_train)

"""Getting the  best models from grid search"""

best_gb_model = grid_search_gb.best_estimator_

best_ab_model = grid_search_ab.best_estimator_

best_rf_model = grid_search_rf.best_estimator_

best_xgb_model = grid_search_xgb.best_estimator_

"""Making predictions on the testing set

"""

predictions_gb = best_gb_model.predict(X_test)

predictions_ab = best_ab_model.predict(X_test)

predictions_rf = best_rf_model.predict(X_test)

predictions_xgb = best_xgb_model.predict(X_test)

"""Calculate evaluation metrics

"""

mae_gb = mean_absolute_error(y_test, predictions_gb)
mae_ab = mean_absolute_error(y_test, predictions_ab)
mae_rf = mean_absolute_error(y_test, predictions_rf)
mae_xgb = mean_absolute_error(y_test, predictions_xgb)
rmse_gb = mean_squared_error(y_test, predictions_gb, squared=False)
rmse_ab = mean_squared_error(y_test, predictions_ab, squared=False)
rmse_rf = mean_squared_error(y_test, predictions_rf, squared=False)
rmse_xgb = mean_squared_error(y_test, predictions_xgb, squared=False)

print(f"Mean Absolute Error (Gradient Boosting): {mae_gb:.2f}")
print(f"Mean Absolute Error (AdaBoost): {mae_ab:.2f}")
print(f"Mean Absolute Error (Random Forest): {mae_rf:.2f}")
print(f"Mean Absolute Error (XGBoost): {mae_xgb:.2f}")
print(f"Root Mean Squared Error (Gradient Boosting): {rmse_gb:.2f}")
print(f"Root Mean Squared Error (AdaBoost): {rmse_ab:.2f}")
print(f"Root Mean Squared Error (Random Forest): {rmse_rf:.2f}")
print(f"Root Mean Squared Error (XGBoost): {rmse_xgb:.2f}")

"""Load the new dataset (players22)

"""

players22=pd.read_csv('/content/drive/My Drive/Colab Notebooks/players_22.csv')

players22.head()

players22.info(verbose=True,null_counts=True)

"""Removing the irrelevant columns"""

irrelevant_columns = ['player_url','player_face_url', 'club_logo_url','club_flag_url','nation_logo_url','nation_flag_url','nationality_id','nationality_name','nation_team_id','club_loaned_from','club_contract_valid_until','club_jersey_number']
players22.drop(columns=irrelevant_columns, inplace=True)

players22=players22.loc[:,players22.isnull().mean() <.3]

"""Separating data, imputing and encoding them respectively"""

numeric_columns = players22.select_dtypes(include=['number'])  # This selects numeric columns
categorical_columns = players22.select_dtypes(exclude=['number'])  # This selects non-numeric (categorical) columns

numeric_columns

categorical_columns

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
numeric_columns = pd.DataFrame(imputer.fit_transform(numeric_columns), columns=numeric_columns.columns)
numeric_columns.info()

categorical_columns.fillna(method='ffill', inplace=True)

from sklearn.preprocessing import LabelEncoder

# Create a LabelEncoder
label_encoder = LabelEncoder()

# Encode each categorical column
for column in categorical_columns:
    categorical_columns[column] = label_encoder.fit_transform(categorical_columns[column])

# Now you can access the encoded DataFrame

encoded=categorical_columns
# Display the first few rows of the encoded DataFrame
encoded.head(30)

"""Combining the numeric alongside the categorical columns"""

# Concatenate them row-wise
combined_df22 = pd.concat([numeric_columns, encoded],axis=1)

# Reset the index if needed
players22 = combined_df22.reset_index(drop=True)

players22.info()

"""Preparing the data for training, finding the top features and scaling"""

# Prepare the data for training
X22 = combined_df22.drop('overall', axis=1)
y22 = combined_df22['overall']

# Select the top features and scale them
X22 = X22[top_features]

scaled_22 = StandardScaler()
X22_scaled = pd.DataFrame(scaled_22.fit_transform(X22), columns=X22.columns)

"""Making predictions based on the datasets and calculating evaluation metrics for the new dataset

"""

predictions_gb = best_gb_model.predict(X22_scaled)
predictions_ab = best_ab_model.predict(X22_scaled)
predictions_rf = best_rf_model.predict(X22_scaled)
predictions_xgb = best_xgb_model.predict(X22_scaled)


mae_gb = mean_absolute_error(y22, predictions_gb)
mae_ab = mean_absolute_error(y22, predictions_ab)
mae_rf = mean_absolute_error(y22, predictions_rf)
mae_xgb = mean_absolute_error(y22, predictions_xgb)
rmse_gb = mean_squared_error(y22, predictions_gb, squared=False)
rmse_ab = mean_squared_error(y22, predictions_ab, squared=False)
rmse_rf = mean_squared_error(y22, predictions_rf, squared=False)
rmse_xgb = mean_squared_error(y22, predictions_xgb, squared=False)

print(f"Mean Absolute Error (Gradient Boosting): {mae_gb:.2f}")
print(f"Mean Absolute Error (AdaBoost): {mae_ab:.2f}")
print(f"Mean Absolute Error (Random Forest): {mae_rf:.2f}")
print(f"Mean Absolute Error (XGBoost): {mae_xgb:.2f}")
print(f"Root Mean Squared Error (Gradient Boosting): {rmse_gb:.2f}")
print(f"Root Mean Squared Error (AdaBoost): {rmse_ab:.2f}")
print(f"Root Mean Squared Error (Random Forest): {rmse_rf:.2f}")
print(f"Root Mean Squared Error (XGBoost): {rmse_xgb:.2f}")

"""We ended up going with random forest Regressor with grid search and cross validation beacuse it gave a lower Mean absolute error value

Saving the best model and the scaler
"""

import joblib
joblib.dump(best_rf_model,'FifaModelPred.pkl')

joblib.dump(scaled_22, 'Scaling.pkl')

X.info()





